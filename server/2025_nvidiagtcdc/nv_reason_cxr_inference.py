import time
import sys
import torch
from transformers import AutoModelForImageTextToText, AutoProcessor, TextIteratorStreamer
from PIL import Image
import itk
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, AsyncGenerator
from threading import Thread
import asyncio

# Import centralized model configuration
sys.path.insert(0, str(Path(__file__).parent.parent))
from scripts.model_paths import MODEL_PATHS

# Global model cache to avoid reloading on every inference
_model_cache: Optional[Tuple[AutoModelForImageTextToText, AutoProcessor]] = None
_model_path = MODEL_PATHS['nv-reason']

def get_model_and_processor() -> Tuple[AutoModelForImageTextToText, AutoProcessor]:
    """Load and cache the model and processor. Reuses cached instances if available."""
    global _model_cache

    if _model_cache is None:
        # Verify model exists
        model_dir = Path(_model_path)
        if not model_dir.exists():
            raise FileNotFoundError(
                f"Model not found at {_model_path}. "
                "Please run model download commands from README.md"
            )

        # Check CUDA availability
        print("\n" + "="*70)
        print("GPU/CUDA Detection")
        print("="*70)
        print(f"PyTorch version: {torch.__version__}")
        print(f"CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"CUDA version: {torch.version.cuda}")
            print(f"GPU device count: {torch.cuda.device_count()}")
            print(f"Current GPU: {torch.cuda.get_device_name(0)}")
            print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        else:
            print("⚠️  WARNING: CUDA not available! Model will run on CPU (very slow)")
            print("Please ensure:")
            print("  1. NVIDIA GPU is installed")
            print("  2. CUDA drivers are installed (nvidia-smi should work)")
            print("  3. PyTorch with CUDA support is installed")
            print("     Run: pip install torch --index-url https://download.pytorch.org/whl/cu121")
        print("="*70 + "\n")

        print(f"Loading NV-Reason-CXR-3B model from {_model_path}...")
        model = AutoModelForImageTextToText.from_pretrained(
            _model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            local_files_only=True,
        ).eval()
        processor = AutoProcessor.from_pretrained(_model_path, use_fast=True, local_files_only=True)

        # Print device mapping
        print("\nModel Device Mapping:")
        if hasattr(model, 'hf_device_map'):
            for name, device in model.hf_device_map.items():
                print(f"  {name}: {device}")
        else:
            print(f"  Model device: {next(model.parameters()).device}")

        _model_cache = (model, processor)
        print("\n✅ NV-Reason-CXR-3B model loaded and cached successfully.\n")

    return _model_cache

async def run_nv_reason_cxr_inference_streaming(
    input_data: Dict[str, Any], itk_img: itk.Image
) -> AsyncGenerator[str, None]:
    """
    Runs inference using the local nvidia-reason-cxr-3b model with streaming support.
    Yields tokens as they are generated by the model.

    Args:
        input_data (Dict[str, Any]): A dictionary containing the user's prompt
                                     under the 'prompt' key and conversation
                                     history under the 'history' key.
        itk_img (itk.Image): An ITK image object of the chest X-ray.

    Yields:
        str: Generated tokens as they become available.
    """
    if itk_img is None:
        raise ValueError("Nvidia CXR model requires an image for analysis.")

    # --- 1. Get Model and Processor (cached) ---
    model, processor = get_model_and_processor()

    # --- 2. Prepare Image ---
    img_array = itk.array_view_from_image(itk_img).squeeze()

    # Normalize to 8-bit integer if not already
    if img_array.dtype != np.uint8:
        numerator = img_array - img_array.min()
        denominator = img_array.max() - img_array.min()
        # Handle the case of a flat image (denominator is zero)
        if denominator > 0:
            img_array = (255 * numerator / denominator).astype(np.uint8)
        else:
            img_array = np.zeros_like(img_array, dtype=np.uint8)

    image = Image.fromarray(img_array).convert("RGB")

    # --- 3. Prepare Input Prompt and History ---
    user_question = input_data.get('prompt', "Find abnormalities and support devices.")
    history = input_data.get('history', [])

    # Build messages from history
    messages = []
    if len(history) > 0:
        valid_index = None
        for i in range(len(history)):
            h = history[i]
            # Check if the content is non-empty
            if len(h.get('content', '').strip()) > 0:
                # Find the first assistant message to determine where to start
                if valid_index is None and h['role'] == 'assistant':
                    valid_index = i - 1
                messages.append({
                    "role": h['role'],
                    "content": [{"type": "text", "text": h['content']}]
                })

        # Remove previous messages without image if needed
        if valid_index is None:
            messages = []
        if len(messages) > 0 and valid_index > 0:
            messages = messages[valid_index:]  # Keep only messages from the first assistant response onwards

    # Add current user prompt
    messages.append({"role": "user", "content": [{"type": "text", "text": user_question}]})

    # Always insert the image at the beginning of the FIRST user message
    # This is critical for the model to have image context throughout the conversation
    messages[0]['content'].insert(0, {"type": "image"})

    # --- 4. Process Inputs for Model ---
    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
    inputs = processor(text=prompt, images=[image], return_tensors="pt")
    inputs = inputs.to(model.device)

    # --- 5. Setup Streaming ---
    # Create streamer with the processor's tokenizer
    # We need to extract the tokenizer properly to avoid path resolution issues
    tokenizer = processor.tokenizer
    streamer = TextIteratorStreamer(
        tokenizer,
        skip_special_tokens=True,
        skip_prompt=True,
        timeout=None  # Don't timeout waiting for tokens
    )

    # Build generation kwargs with all inputs plus streaming config
    generation_kwargs = {
        **inputs,
        "max_new_tokens": 4096,
        "streamer": streamer,
        "do_sample": False,  # Ensure consistent generation
    }

    # --- 6. Start Generation in Thread ---
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    # --- 7. Yield Tokens as They Arrive ---
    try:
        for token_text in streamer:
            if token_text:  # Only yield non-empty tokens
                yield token_text
            # Force yielding control to event loop
            await asyncio.sleep(0)
    finally:
        # Wait for generation thread to complete
        thread.join()

        # --- 8. Clean Up Memory (but keep model cached) ---
        del inputs
        # Only try to empty cache if CUDA is available
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
